---

---
title: "Statistics for Data Science - Homework 2"
author: "Barba Paolo, Candi Matteo"
output: html_document
---
<style type="text/css">
body, td {
   font-size: 14px;
}
code.r{
  font-size: 12px;
}
pre {
  font-size: 12px
}
</style>
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Packages}
rm(list = ls())
# Load packages.
packs <- c('MASS', 'pracma', 'psych')
lapply(packs, require, character.only = TRUE)
```
## Purpose and Statistical tools used

The goal of the project is to perform Friedman's procedure as a proxy of
multiple studies and applyi it on fMRI (functional magnetic resonance
imaging) data in order to test wheter the data from td (tipacally
develpment) subjects and asd (authism spectrum disorder) subject came
from the same distribution or not. In order to reach this goal we used
statistical tools as machine learning algorithm and two-sample
Hypothesis testing.

## Exercise 3

Friedman's procedure is used for solving two-sample hypothesis testing
when the each observation consists of many measured attributes
$x_{i} = x_{i_1}, x_{i_2} . . . x_{i_{M}}$ and
$z_{i} = z_{i_1}, z_{i_2} . . . z_{i_{M}}$ where M is the dimensionality
of the dataset and we have $n_{0}$ samples for $\underline{x}$ and
$n_{1}$ samples for $\underline{z}$. We create then a hypothesis system
as the following:

\begin{equation}

\begin{cases}

H_{0}: F_{x} = F_{z}\\ H_{1}: F_{x} \neq F_{z}

\end{cases}

\end{equation}

Since the distribution we want to compare are multivariate ones, there are not variety of procedures that can compare them directly. The idea then, is to perform classification machine learning algorithm as logistic regression. Then compute the scores of the observed data $\underline{s_{x}}$ and  $\underline{s_{z}}$ and compare the univariate distribuion of them using two sample hypothesis test as Kolmogorov-Smirnov. So the hypothesis system would be as the following:


\begin{equation}

\begin{cases}

H_{0}: F_{s_{x}} = F_{s_{z}}\\ H_{1}: F_{s_{x}} \neq F_{s_{z}}

\end{cases}

\end{equation}


When the same data is used for both training and subsequent scoring, these univariate null distributions are not valid.
To solve this Friedman had proposed the following procedure to compute the distribution of the statistical test under H0:
Suppose that the sample $z_{i}$ came from a reference distribution (say $p_{0}$).


• Drawn a sample of size n1 from p0.

• Use them with the actual data to train the classification model and compute the scores.

• Compute the statistics test between the two scores sample.

• Repeat P times.








```{r parameters}
k <- 5    # Dimensions
n0 <- 80  # Sample size n0
n1 <- 80  # Sample size n1
alpha <- .05  # significance level
set.seed(123) # Reproducibility 
```

```{r sample normal}
Take_sample_normal <- function(n, mu, sigma, label){
  x <- mvrnorm(n, mu, sigma)
  x <- as.data.frame(cbind(x, label))
  colnames(x[length(colnames(x))]) <- "label"
  return(x)
}
mu <- rep(1,k) # Vector of mu
sigma <- matrix(0, ncol = k , nrow = k)  # Varianza covariance matrix
diag <- diag(1,k)
sigma <- sigma + diag
```

```{r sample normal}
sigmoid <- function(x, theta){
  n_features <- length(theta)
  n <- exp(theta[1] + sum(x[-length(x)]*theta[2:n_features]))
  
  return(n / (1+n))
}
```
In order to get the distribution of the Kolmogorov-Smirnov statistic under the null hypopthesis, we implement the Friedman's procedure, supposing a reference distribution $p_{0}$ that have generated the $\underline{z}$ sample. Under the null hypothesis $p_{0}$ has the same distribution than the one that have generated the $\underline{x}$ sample. After have implemented the Friedmann's procedure we end with $t_{1} \dots t_{P}$ kolmogorov statistics that can be used as the distribution of statistic test under the null hypothesis. So we can build the reject region $R_{\alpha}$ as the values of the test statistic  greater than a threshold equal $1 - \alpha$ quantile of the distribution. We reject with significance level $\alpha$ if the observed statistics is greater than the threshold.

```{r Friedmann Procedure}
P <- 1000

x <- Take_sample_normal(n = n0, mu = mu, sigma = sigma, label = 0)
z <- Take_sample_normal(n = n1, mu = mu, sigma = sigma, label = 1)

Friedman_procedure <- function(P){
  
  kolm_t <- rep(NA, P)
  
  for(i in 1:P){
    z_p <- Take_sample_normal(n = n1, mu = mu, sigma = sigma, label = 1) # Under H_0
    u_p <- rbind(x, z_p)
    glm_coef <- unname(glm(label ~ ., data = u_p)$coefficients)
    x_scores <- apply(x, MARGIN = 1, sigmoid, theta = glm_coef)
    z_scores <- apply(z, MARGIN = 1, sigmoid, theta = glm_coef) 
    kolm_t[i] <- ks.test(x_scores, z_scores, alternative = "two.sided")$statistic
  }
  kolm_t <<- kolm_t
  
  hist(kolm_t, main = "
       Kolmogorov-Smirnov statistic \n distribution under H_0",
       col = "skyblue", border = "white", breaks= 30)
  p_kolm <<- quantile(kolm_t , 1 - alpha)
  abline(v = p_kolm , col = "red" , lty = 3 , lwd = 2)
  box()
}

Friedman_procedure(P)

```

