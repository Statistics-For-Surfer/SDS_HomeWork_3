------------------------------------------------------------------------

---
title: "Statistics for Data Science - Homework 3"
author: "Barba Paolo, Candi Matteo"
output: html_document
---

```{=html}
<style type="text/css">
body, td {
   font-size: 14px;
}
code.r{
  font-size: 12px;
}
pre {
  font-size: 12px
}
</style>
```

------------------------------------------------------------------------

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Packages , warning = FALSE , message = FALSE , results='hide' , echo = FALSE}
rm(list = ls())
#load packages
packs <- c('MASS', 'pracma', 'psych','caTools','e1071','dgof','latex2exp')
lapply(packs, require, character.only = TRUE)
```

## Purpose and Statistical tools used

The goal of the project is to perform Friedman's procedure as a proxy of multiple studies and apply it on fMRI (functional magnetic resonance imaging) data in order to test wheter the data from td (tipacally develpment) subjects and asd (authism spectrum disorder) subject came from the same distribution or not. In order to reach this goal we used statistical tools as machine learning algorithm and two-sample Hypothesis testing.

## Exercise 3

### Two sample hypotesis test:

Friedman's procedure is used for solving two-sample hypothesis testing when the each observation consists of many measured attributes $x_{i} = x_{i_1}, x_{i_2} . . . x_{i_{M}}$ and $z_{i} = z_{i_1}, z_{i_2} . . . z_{i_{M}}$ where M is the dimensionality of the dataset and we have $n_{0}$ samples for $\underline{x}$ and $n_{1}$ samples for $\underline{z}$. We create then a hypothesis system as the following:

```{=tex}
\begin{equation}

\begin{cases}

H_{0}: F_{x} = F_{z}\\ H_{1}: F_{x} \neq F_{z}

\end{cases}

\end{equation}
```
Since the distribution we want to compare are multivariate ones, there are not variety of procedures that can compare them directly. The idea then, is to perform classification machine learning algorithm as logistic regression, compute the scores of the observed data $\underline{s_{x}}$ and $\underline{s_{z}}$ and compare the univariate distribuion using two sample hypothesis test as Kolmogorov-Smirnov test. So the hypothesis system would be as the following:

```{=tex}
\begin{equation}

\begin{cases}

H_{0}: F_{s_{x}} = F_{s_{z}}\\ H_{1}: F_{s_{x}} \neq F_{s_{z}}

\end{cases}

\end{equation}
```
The take out the distribution under the null hypothesis we lead the following procedure


<div>
<ol>
    <li> Randomly permute the group label </li>
   
    <li> Train classifier (Logistic regression in our case) </li>
   
    <li> Compute the scores of the observations </li>
   
    <li> Compute the test statistic (Kolmogorov-Smirnov in our case) </li>
   
    <li> Repeat P times </li>
</ol>
</dic>


After this procedure, we obtain a set of statistics test $\underline{t} = {t_{1} \dots t_{n}}$.

Under $H_{0}$, the entire data vector ${\underline{x} ,\underline{z}}$ is an IID sample from a single distribution F, and so the group labels are meaningless.

Then the observed statistics $\hat{t}$ is equally likely to be anywhere in $\underline{t}$.

We can compute the p-value $p = \frac{1}{N} \sum_{j=1}^N \mathbb{I}(T_j \geqslant T) = \{\text{proportion of permuted statistics larger than the original}\}$ setting the decision rule in order to reject the null hypothesis if the value of $p$ is lower than $\alpha$.

### Goodness of fit test:

The two sample hypothesis test can be turn to a goodness of fit test. Here we have the assumptions that the sample $\underline{z}$ came from an reference distribution (Say $F$). We can set up the following hypothesis system:

```{=tex}
\begin{equation}

\begin{cases}

H_{0}: F_{x} = F\\ H_{1}: F_{x} \neq F

\end{cases}

\end{equation}
```
Here we got only one sample $\underline{x} = \{ x_{i1} \dots x_{iM} \}_{i =1}^{N}$ and we want to test if this sample came from the reference distribution $F$. The distributions are still multivariate and we can use a classifiers to reduce a multivariate goodness of fit test into a univariate one. The hypothesis system change as the following:

```{=tex}
\begin{equation}

\begin{cases}

H_{0}: F_{s_{x}} = F_{s}\\ H_{1}: F_{s_{x}} \neq F_{s}

\end{cases}

\end{equation}
```
To perform the above test we can use kolmogorov-Smirnov test and to get out the distribution of the kolmogorov statistics under the null hypothesis we can set up a Monte Carlo simulation as the following:
<div>
<ol> 
    <li> Drawn a sample $z_{i}$ of size n1 from p0. </li>

    <li> Use them with the actual data to train the classification model and compute the scores. </li>

    <li>Compute the statistics test between the two scores sample.</li>

    <li> Repeat P times. </li>
</ol>
</dic>

After this procedure, we obtain a set of statistics test $\underline{t} = {t_{1} \dots t_{n}}$ that can be used as the distribution of statistic test under the null hypothesis. So we can build the reject region $R_{\alpha}$ as the values of the test statistic greater than a threshold equal $1 - \alpha$ quantile of the distribution. We reject with significance level $\alpha$ if the observed statistics is greater than the threshold.

We can compute the p-value $p = \frac{1}{N} \sum_{j=1}^N \mathbb{I}(T_j \geqslant T) = \{\text{proportion of permuted statistics larger than the original}\}$ setting the decision rule in order to reject the null hypothesis if the value of $p$ is lower than $\alpha$.

```{r parameters}
k <- 5    # Dimensions
n0 <- 70  # Sample size 0
n1 <- 70  # Sample size 1
alpha <- .05  # significance level
set.seed(324) # reproducibility
P <- 100      # Simulation size
acc_rej_col <- c("#d3305d" , "#ABCDEF")   # Color palette
```

```{r sample normal}
# Take random sample from a multi-normal distribution.
Take_sample_normal <- function(n, mu, sigma, label){
  x <- mvrnorm(n, mu, sigma)    # Random generate from a multivariate normal 
  x <- as.data.frame(cbind(x, label))   # add label
  colnames(x[length(colnames(x))]) <- "label"   # col label
  
  return(x)
}

mu <- rep(0,k)    # Mean <- c(0,0,0,0,0 ...)
sigma <- diag(1 ,k)  # Identity matrix for sigma 

```


```{r Friedman_procedure , warning=FALSE}
Friedman_procedure <- function(P,x_data , z_data , permut = FALSE){
  x_fri <- x_data       # Copy the data
  z_p <- z_data         # copy the z data
  kolm_t <- rep(NA, P)  # Pre-set the Kolmogorov-Smirnov statistic
  labels <- c(rep(0,n1),rep(1,n1))   # Set of the label
  for(i in 1:P){                     # Loop 
    if(permut == F){                 # Check goodnees of fit
      z_p <- Take_sample_normal(n1, mu, sigma, label =1)  # Sample under the null F
      } 
    if(permut == T){                 # Two sample test
    idx <- sample(x = 1:(n0+n1), n0+n1)    # Shuffle the label
    
    x_fri$label <- labels[idx[1:n0]]            # Permuted label
    z_p$label <- labels[idx[(n0+1):(n0+n1)]]}   # Permuted label
    u_p <- as.data.frame(rbind(x_fri,z_p))      # Row bind the two data frame
    glm_f <- glm(label~., data = u_p)           # Train the model
    scores <- predict(glm_f ,u_p[,1:k])         # Compute the scores
    
    kolm_t[i] <- ks.test(scores[u_p$label == 0] , scores[u_p$label == 1])$statistic  
  }
  return(kolm_t)

}
```



```{r alpha info, warning=FALSE , eval = FALSE}

alpha_info <- function(P , permut = FALSE){
  prop_rej <- rep(NA, P)    # Pre-set accept/reject values
  p_values <- rep(NA , P)   # Preset pvalues
  
  for(i in 1:P){            # Loop over P
    x_p <- Take_sample_normal(n = n0, mu = mu, sigma = sigma, label = 0)  # Take sample of
    z_p <- Take_sample_normal(n = n1, mu = mu, sigma = sigma, label = 1)  # same distributions
    u_p<- rbind(x_p, z_p) # Combine the data
    if(permut == FALSE){   # Goodness of fit test
      kk <- Friedman_procedure(P, x_data = x_p , z_data = z_p)}
    if(permut == TRUE){    # Two sample test
      kk <- Friedman_procedure(P, x_data = x_p , z_data = z_p , permut = T)
    }
    
    glm_model <-glm(label ~ ., data = u_p)   # Perform the glm observed model
    
    x_scores <- predict(glm_model , x_p[,1:k])   # Compute the obs score
    z_scores <- predict(glm_model,  z_p[,1:k])   # ""
    
    true_kolm <- ks.test(x_scores, z_scores, alternative = "two.sided")$statistic  # observed statistic
    prop_rej[i] <- true_kolm < quantile(kk , 1 -alpha)   # check if accept or no
    p_values[i] <- sum(kk > true_kolm ) / length(kk)     # Store the p-values

  }
  
  data = as.data.frame(cbind(prop_rej,p_values))        # combine the dataframe
  colnames(data) <- c("KS","p_values")                  # Change the names
  return(data)
}
```

```{r alphasss , echo = FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
load("data/alpha_info_1.RData")
t <- proportions(table(data[,1]))
barplot(t, col = acc_rej_col , main = TeX(r"(Proportion of times we accept / reject $H_{0}$ when is True)" , bold = T) ,cex.main = .8, names.arg = c("Reject" , "Accept"), ylim = c(0,1) , las = 1)

```


```{r p-value ,echo = FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
hist(data[,2],freq = F , breaks = 6 , border = "white",
      col = "#B6FCD5", main = TeX(r"(pvalue distribution under $H_{0}$ )", bold = T) ,cex.main = .8, xlab = TeX(r"(\hat{p}value)"))
points(seq(0,1,length.out = 300) , rep(1,300) , col = "#C6E2FF" , cex = .8)
box()
legend("topright" , legend = "Uniform distribution" , border = "white" , lty = 1, col = "#C6E2FF" , bty = "n", cex = .8, lwd = 2)
?legend

```

### Comments

In the simulation above we are in a scenario were $n_{0} = n{1} = 70$, the dimensionality $k = 5$ and the simulation size $P = 100$ and we are dealing with goodness of fit test.
First plot show the empirical size $\alpha \approx 0.05$ and the second one show the distribution of the observed $\hat{p}$ values with the theretical $Unif(0,1)$ distribution. Increasing $P$ it would fit better.

The real goal of this type of simulations is to see how an algorithm would perform w.r.t to metrics as size and power while changing input parameters as $n_{0}, n_{1}, k$.

### Performance with rispect to $\alpha$
```{r changing alpha, changing n0, warning=FALSE , eval = FALSE}

n0s <- c(20,50,70,100)   # Values of alpha tested
k <- 5                   # Set the dimensionality as 5
P <- 150
alpha_Friedmann <- rep(NA ,length(n0s))    # Preset the empirical alpha
alpha_per <- rep(NA , length(n0s))         # Preset the empirical alpha 
for(i in 1:length(n0s)){
  n0 <- n0s[i] 
  alpha_Friedmann[i] <- 1-mean(alpha_info(P)$KS)
  alpha_per[i] <- 1-mean(alpha_info(P,permut = T)$KS)
  }

```

```{r plots change size ,fig.height = 4, fig.width = 5, fig.align = "center"   ,echo = FALSE}
load("alpha_fr_change_n.RData")
load("alpha_per_change_n.RData")
n0s <- c(20,50,70,100)   # Values of alpha tested

{plot(n0s ,alpha_Friedmann , type = "l" , ylim = c(0,0.2) , lwd = 2 , ylab = TeX(r"($\alpha$)")  ,xlab = TeX(r"(sample size $n_{0}$)") , col = "#2274A5" , main = TeX(r"($alpha$ vs $n_{0}$ with $k = 5$ and $n_{1} = 70$)", bold = T))
points(n0s , alpha_per , type = "l" , col = "#E7EB90" , lwd = 2)

legend("topright" , legend = c("Goodness of fit test", "Two sample test") ,
       col = c("#2274A5","#E7EB90") , border = "white" ,
       lty = 1 , bty = "n", 
       cex =.8, lwd = 2)
grid()}

```

```{r changing alpha, changing, warning=FALSE , eval = FALSE}
ks <- c(5,20,70,100)
P <- 20
n0 <- 70
alpha_permutation_2 <- rep(NA ,length(ks))
alpha_Friedmann_2 <- rep(NA ,length(ks))
for(i in 1:length(ks)){
  print(i)
  k <- ks[i]
  mu <- rep(0,k)
  sigma <- diag(1,k)
  alpha_permutation_2[i] <- 1-mean(alpha_info(P)$KS)
  alpha_Friedmann_2[i] <- 1-mean(alpha_info(P,permut = T)$KS)
  }
  
```


```{r plots alpha changing k, warning=FALSE , fig.height = 4, fig.width = 5, fig.align = "center" ,echo = FALSE}

load("data/alpha_per_change_k.RData")
load("data/alpha_fr_change_k.RData")
ks <- c(5,20,70,100)

{plot(ks ,alpha_permutation_2 , type = "l" , ylim = c(0,0.2) , lwd = 2 , ylab = TeX(r"($\alpha$)")  ,xlab = TeX(r"(Dimensionality $k$)") , col = "#2274A5" , main = TeX(r"($alpha$ vs $k$ with $n_{0} = 70$ and $n_{1} = 70$)", bold = T))
points(ks , alpha_Friedmann_2 , type = "l" , col = "#E7EB90" , lwd = 2)

legend("topright" , legend = c("Goodness of fit test", "Two sample test") ,
       col = c("#2274A5","#E7EB90") , border = "white" ,
       lty = 1 , bty = "n", 
       cex =.8, lwd = 2)
grid()}

```
#### Comments
[TODO] grafici con P = 100 

### Performance with rispect to the power $1 - \beta$

To get out information about the power of the test (probability to get a correct discovery) we set up a simulation generating sample from difference distribution to check how likely it is that our test rejects $H_{0}$ when it is false. We set up different scenarios where $F_{x} \neq F_{z}$ to generate the data under $H_{0}$ and count how many times we reject the null hypothesis. Of course, if the two distributions are close each other is more difficult to discover the difference and reject $H_{0}$ then.

In the simulation study we choose Gaussian parametric family for both $F_{x}$ and $F_{z}$, where $F_{z}$ has diffenent position parametes $\underline{\mu}$. We chose closed-form of Wasserstein distance as a distance $D(F_{x},F_{z}) = D(\underline{\mu_{x}} ,\underline{\mu_{z}})$ .
$$
\boldsymbol{X} \sim \mathrm{N}_k(\boldsymbol{\mu}_1, \Sigma_1)
\hspace{.3cm}
\boldsymbol{Y} \sim \mathrm{N}_k(\boldsymbol{\mu}_2, \Sigma_2)
$$


$$
\textsf{W}^2_2(\boldsymbol{X}, \boldsymbol{Y}) = \| \boldsymbol{\mu}_1 - \boldsymbol{\mu}_2\|_2^2 + \mathrm{B}^2(\Sigma_1, \Sigma_2)
$$


```{r distance function, warning=FALSE}
# Distance functions.
beta_q <- function(sigma1, sigma2){
  trace_1 <- tr(sigma)    # Trace of sigma X
  trace_2 <- tr(sigma2)   # Trace of sigma Y
  trace_3 <- tr(sqrtm(sqrtm(sigma1)$B %*% sigma2 %*% sqrtm(sigma1)$B)$B)
  
  return(trace_1 + trace_2 - 2*trace_3)
}

Wasserstein_distance <- function(mu1, mu2, sigma1, sigma2){
  norma <- norm(mu1-mu2, type = "2")**2   
  beta_quadro <- beta_q(sigma1, sigma2)
  
  return(norma + beta_quadro)
}

```

```{r power info , warning = FALSE}
power_info <- function(P, k, increment , permut = F){
  
  prop_ks <- rep(NA, P)    # Pre-set proportions of accept/reject
  for(i in 1:P){
    x <- Take_sample_normal(n = n0, mu= mu, sigma = sigma, label = 0)  # Take sample of x
    z <- Take_sample_normal(n = n1, mu=  mu2, sigma = sigma2, label = 1) # Take sample of z (From a different distribution)
    u <- rbind(x,z)    # Combine the dataset
    p_model <- glm(label ~ ., data = u)   # Perform the obs model
    x_scores <- predict(p_model, x)       # Predict the obs scores
    z_scores <- predict(p_model, z)       # ""
    kolm_obs <- ks.test(x_scores, z_scores, alternative = "two.sided")$statistic                    # Obs statistic
    if(permut == FALSE){
      kk <- Friedman_procedure(P,x_data = x, z_data = z)
    }
    if(permut == TRUE){
      kk <- Friedman_procedure(P,x_data = x, z_data = z , permut = T)
    }
  
    prop_ks[i] <- kolm_obs > quantile(kk,1-alpha)
  }
  data <-  mean(prop_ks)
  
  
  return(data)
}

```

```{r power info changing distance , warning = FALSE , eval = FALSE}
max_in <- .9
P <- 100
k <- 5
n0 <- 30
n1 <- 20
mu <- rep(0,k)
sigma <- diag(1,k)
ll <- seq(from = 0 , to = max_in , by = 0.1)
powers <- c()
powers_2 <- c()
distance <- c()
for (i in (ll)){
  mu2 <- mu + i
  print(mu2)
  sigma2 <- sigma
  distance <- c(distance,Wasserstein_distance(mu, mu2, sigma, sigma2))
  powers <- c(powers,power_info(P, k = length(mu),increment = i))
  powers_2 <- c(powers_2 ,power_info(P, k = length(mu),increment = i , permut = T))
}


```

```{r plot power info twosamplegof , , fig.height = 4, fig.width = 5, fig.align = "center" ,echo = FALSE}

load("data/power_GoF.RData")
load("data/power_two_sample_test.RData")
load("data/distance_Gof_two.RData")
plot(distance,powers, main = TeX(r"(Power vs Distance $W_{2}^{2}(X,Z)$)" , bold = T), xlab = "Distance", ylab = "power",
     col = 'darkblue' , type = "o" , 
     lwd = 2)
points(distance, powers_2 , col = "darkred" , type = "o" , lwd = 2)
legend("bottomright" , legend = c("GoF", "Two-sample") , col = c("darkblue","darkred"),border = "white" ,
       lty = 1 , bty = "n", 
       cex =.8, lwd = 2)
grid()
```


### Comments
In the Goodness of fit hypothesis testing we use the additional information of the distribution of $F_{z} = F$, such of that, this procedure has the potential to increase the power of the test. However, in the real case we simply got two sample without having this info..[TODO] continua


### Two sample hypothesis test power vs dimensionality
[TODO]

### Two sample hypothesis test power vs sample size
[TODO]


# Exercise 4






